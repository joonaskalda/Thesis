{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "import itertools\n",
    "import pickle, gzip\n",
    "import glob\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,BatchNormalization,Dropout\n",
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from collections import Counter\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../input/training_set.csv' does not exist: b'../input/training_set.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-9e510c953937>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtrain\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../input/training_set.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'flux_ratio_sq'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'flux'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'flux_err'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'flux_by_flux_ratio_sq'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'flux'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'flux_ratio_sq'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 702\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 429\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 895\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    896\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1120\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1121\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1122\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1123\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1124\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1853\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1855\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../input/training_set.csv' does not exist: b'../input/training_set.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "gc.enable()\n",
    "\n",
    "train = pd.read_csv('../input/training_set.csv')\n",
    "train['flux_ratio_sq'] = np.power(train['flux'] / train['flux_err'], 2.0)\n",
    "train['flux_by_flux_ratio_sq'] = train['flux'] * train['flux_ratio_sq']\n",
    "\n",
    "\n",
    "aggs = {\n",
    "    'mjd': ['min', 'max', 'size'],\n",
    "    'passband': ['min', 'max', 'mean', 'median', 'std'],\n",
    "    'flux': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "    'flux_err': ['min', 'max', 'mean', 'median', 'std','skew'],\n",
    "    'detected': ['mean'],\n",
    "    'flux_ratio_sq':['sum','skew'],\n",
    "    'flux_by_flux_ratio_sq':['sum','skew'],\n",
    "}\n",
    "\n",
    "agg_train = train.groupby('object_id').agg(aggs)\n",
    "new_columns = [\n",
    "    k + '_' + agg for k in aggs.keys() for agg in aggs[k]\n",
    "]\n",
    "agg_train.columns = new_columns\n",
    "agg_train['mjd_diff'] = agg_train['mjd_max'] - agg_train['mjd_min']\n",
    "agg_train['flux_diff'] = agg_train['flux_max'] - agg_train['flux_min']\n",
    "\n",
    "\n",
    "del agg_train['mjd_max'], agg_train['mjd_min']\n",
    "agg_train.head()\n",
    "\n",
    "del train\n",
    "gc.collect()\n",
    "\n",
    "meta_train = pd.read_csv('../input/training_set_metadata.csv')\n",
    "meta_train.head()\n",
    "\n",
    "full_train = agg_train.reset_index().merge(\n",
    "    right=meta_train,\n",
    "    how='outer',\n",
    "    on='object_id'\n",
    ")\n",
    "\n",
    "if 'target' in full_train:\n",
    "    y = full_train['target']\n",
    "    del full_train['target']\n",
    "classes = sorted(y.unique())\n",
    "\n",
    "print('Unique classes : ', classes)\n",
    "\n",
    "if 'object_id' in full_train:\n",
    "    oof_df = full_train[['object_id']]\n",
    "    del full_train['object_id'], full_train['distmod'], full_train['hostgal_specz']\n",
    "    del full_train['ra'], full_train['decl'], full_train['gal_l'],full_train['gal_b'],full_train['ddf']\n",
    "\n",
    "\n",
    "train_mean = full_train.mean(axis=0)\n",
    "full_train.fillna(train_mean, inplace=True)\n",
    "\n",
    "folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)\n",
    "\n",
    "full_train_new = full_train.copy()\n",
    "ss = StandardScaler()\n",
    "full_train_ss = ss.fit_transform(full_train_new)\n",
    "\n",
    "unique_y = np.unique(y)\n",
    "class_map = dict()\n",
    "for i,val in enumerate(unique_y):\n",
    "    class_map[val] = i\n",
    "        \n",
    "y_map = np.zeros((y.shape[0],))\n",
    "y_map = np.array([class_map[val] for val in y])\n",
    "y_categorical = to_categorical(y_map)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "def lgb_multi_weighted_logloss(y_true, y_preds):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    # class_weights taken from Giba's topic : https://www.kaggle.com/titericz\n",
    "    # https://www.kaggle.com/c/PLAsTiCC-2018/discussion/67194\n",
    "    # with Kyle Boone's post https://www.kaggle.com/kyleboone\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    if len(np.unique(y_true)) > 14:\n",
    "        classes.append(99)\n",
    "        class_weight[99] = 2\n",
    "    y_p = y_preds.reshape(y_true.shape[0], len(classes), order='F')\n",
    "\n",
    "    # Trasform y_true in dummies\n",
    "    y_ohe = pd.get_dummies(y_true)\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1 - 1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe.values * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).values.astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return 'wloss', loss, False\n",
    "\n",
    "# https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69795\n",
    "def mywloss(y_true,y_pred):\n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss\n",
    "\n",
    "def multi_weighted_logloss(y_ohe, y_p):\n",
    "    \"\"\"\n",
    "    @author olivier https://www.kaggle.com/ogrellier\n",
    "    multi logloss for PLAsTiCC challenge\n",
    "    \"\"\"\n",
    "    classes = [6, 15, 16, 42, 52, 53, 62, 64, 65, 67, 88, 90, 92, 95]\n",
    "    class_weight = {6: 1, 15: 2, 16: 1, 42: 1, 52: 1, 53: 1, 62: 1, 64: 2, 65: 1, 67: 1, 88: 1, 90: 1, 92: 1, 95: 1}\n",
    "    # Normalize rows and limit y_preds to 1e-15, 1-1e-15\n",
    "    y_p = np.clip(a=y_p, a_min=1e-15, a_max=1-1e-15)\n",
    "    # Transform to log\n",
    "    y_p_log = np.log(y_p)\n",
    "    # Get the log for ones, .values is used to drop the index of DataFrames\n",
    "    # Exclude class 99 for now, since there is no class99 in the training set\n",
    "    # we gave a special process for that class\n",
    "    y_log_ones = np.sum(y_ohe * y_p_log, axis=0)\n",
    "    # Get the number of positives for each class\n",
    "    nb_pos = y_ohe.sum(axis=0).astype(float)\n",
    "    # Weight average and divide by the number of positives\n",
    "    class_arr = np.array([class_weight[k] for k in sorted(class_weight.keys())])\n",
    "    y_w = y_log_ones * class_arr / nb_pos\n",
    "    loss = - np.sum(y_w) / np.sum(class_arr)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 10  3 ...  2  8  0]\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.970445\ttraining's wloss: 0.98527\tvalid_1's multi_logloss: 1.30144\tvalid_1's wloss: 1.13674\n",
      "[200]\ttraining's multi_logloss: 0.741601\ttraining's wloss: 0.752563\tvalid_1's multi_logloss: 1.13397\tvalid_1's wloss: 0.986662\n",
      "[300]\ttraining's multi_logloss: 0.630888\ttraining's wloss: 0.638871\tvalid_1's multi_logloss: 1.07294\tvalid_1's wloss: 0.940065\n",
      "[400]\ttraining's multi_logloss: 0.557153\ttraining's wloss: 0.563209\tvalid_1's multi_logloss: 1.04105\tvalid_1's wloss: 0.930296\n",
      "Early stopping, best iteration is:\n",
      "[437]\ttraining's multi_logloss: 0.534148\ttraining's wloss: 0.539821\tvalid_1's multi_logloss: 1.03223\tvalid_1's wloss: 0.929929\n",
      "0.9299291038440487\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.975235\ttraining's wloss: 0.993964\tvalid_1's multi_logloss: 1.2872\tvalid_1's wloss: 1.14034\n",
      "[200]\ttraining's multi_logloss: 0.744093\ttraining's wloss: 0.757921\tvalid_1's multi_logloss: 1.11389\tvalid_1's wloss: 0.996858\n",
      "[300]\ttraining's multi_logloss: 0.629563\ttraining's wloss: 0.639167\tvalid_1's multi_logloss: 1.04745\tvalid_1's wloss: 0.977776\n",
      "Early stopping, best iteration is:\n",
      "[286]\ttraining's multi_logloss: 0.642519\ttraining's wloss: 0.652514\tvalid_1's multi_logloss: 1.05358\tvalid_1's wloss: 0.976086\n",
      "0.9760857353400546\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.969185\ttraining's wloss: 0.987083\tvalid_1's multi_logloss: 1.31214\tvalid_1's wloss: 1.14032\n",
      "[200]\ttraining's multi_logloss: 0.738315\ttraining's wloss: 0.751785\tvalid_1's multi_logloss: 1.14048\tvalid_1's wloss: 0.966562\n",
      "[300]\ttraining's multi_logloss: 0.62406\ttraining's wloss: 0.633863\tvalid_1's multi_logloss: 1.07567\tvalid_1's wloss: 0.921811\n",
      "[400]\ttraining's multi_logloss: 0.54958\ttraining's wloss: 0.557841\tvalid_1's multi_logloss: 1.04076\tvalid_1's wloss: 0.904757\n",
      "Early stopping, best iteration is:\n",
      "[417]\ttraining's multi_logloss: 0.539142\ttraining's wloss: 0.547181\tvalid_1's multi_logloss: 1.03582\tvalid_1's wloss: 0.903589\n",
      "0.9035893704032408\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.972075\ttraining's wloss: 0.98883\tvalid_1's multi_logloss: 1.30403\tvalid_1's wloss: 1.13776\n",
      "[200]\ttraining's multi_logloss: 0.739393\ttraining's wloss: 0.750095\tvalid_1's multi_logloss: 1.13438\tvalid_1's wloss: 0.978216\n",
      "[300]\ttraining's multi_logloss: 0.624089\ttraining's wloss: 0.63225\tvalid_1's multi_logloss: 1.07469\tvalid_1's wloss: 0.95752\n",
      "Early stopping, best iteration is:\n",
      "[321]\ttraining's multi_logloss: 0.605613\ttraining's wloss: 0.613431\tvalid_1's multi_logloss: 1.06551\tvalid_1's wloss: 0.955892\n",
      "0.9558922061920191\n",
      "Training until validation scores don't improve for 50 rounds.\n",
      "[100]\ttraining's multi_logloss: 0.970882\ttraining's wloss: 0.986117\tvalid_1's multi_logloss: 1.30163\tvalid_1's wloss: 1.13959\n",
      "[200]\ttraining's multi_logloss: 0.743397\ttraining's wloss: 0.754185\tvalid_1's multi_logloss: 1.12094\tvalid_1's wloss: 0.964068\n",
      "[300]\ttraining's multi_logloss: 0.629679\ttraining's wloss: 0.638364\tvalid_1's multi_logloss: 1.05191\tvalid_1's wloss: 0.913351\n",
      "[400]\ttraining's multi_logloss: 0.554714\ttraining's wloss: 0.56229\tvalid_1's multi_logloss: 1.01703\tvalid_1's wloss: 0.904989\n",
      "[500]\ttraining's multi_logloss: 0.499054\ttraining's wloss: 0.505765\tvalid_1's multi_logloss: 0.991431\tvalid_1's wloss: 0.904124\n",
      "Early stopping, best iteration is:\n",
      "[473]\ttraining's multi_logloss: 0.512752\ttraining's wloss: 0.51978\tvalid_1's multi_logloss: 0.997391\tvalid_1's wloss: 0.902461\n",
      "0.902461416074561\n",
      "LGBM MULTI WEIGHTED LOG LOSS : 0.93377 \n"
     ]
    }
   ],
   "source": [
    "#Here we train LGBM\n",
    "# Compute weights\n",
    "w = y.value_counts()\n",
    "weights = {i : np.sum(w) / w[i] for i in w.index}\n",
    "\n",
    "print(y_map)\n",
    "clfs_l = []\n",
    "oof_preds_l = np.zeros((len(full_train_ss), len(classes)))\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y_map, y_map)):\n",
    "    checkPoint = ModelCheckpoint(\"./keras.model\",monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\n",
    "    x_train, y_train, y_train_og = full_train_ss[trn_], y_categorical[trn_], y[trn_]\n",
    "    x_valid, y_valid, y_valid_og = full_train_ss[val_], y_categorical[val_], y[val_]\n",
    "    \n",
    "    lgb_params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'multiclass',\n",
    "        'num_class': 14,\n",
    "        'metric': 'multi_logloss',\n",
    "        'learning_rate': 0.03,\n",
    "        'subsample': .9,\n",
    "        'colsample_bytree': .7,\n",
    "        'reg_alpha': .01,\n",
    "        'reg_lambda': .01,\n",
    "        'min_split_gain': 0.01,\n",
    "        'min_child_weight': 10,\n",
    "        'n_estimators': 1000,\n",
    "        'silent': -1,\n",
    "        'verbose': -1,\n",
    "        'max_depth': 3\n",
    "    }\n",
    "    model = lgb.LGBMClassifier(**lgb_params)\n",
    "\n",
    "    model.fit(\n",
    "        x_train,y_train_og,\n",
    "        eval_set=[(x_train,y_train_og), (x_valid, y_valid_og)],\n",
    "        eval_metric=lgb_multi_weighted_logloss,\n",
    "        verbose=100,\n",
    "        early_stopping_rounds=50,\n",
    "        sample_weight=y_train_og.map(weights)\n",
    "    )\n",
    "    oof_preds_l[val_, :] = model.predict_proba(x_valid, num_iteration=model.best_iteration_)\n",
    "    \n",
    "    print(multi_weighted_logloss(y_valid, model.predict_proba(x_valid, num_iteration=model.best_iteration_)))\n",
    "    clfs_l.append(model)\n",
    "    \n",
    "print('LGBM MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_categorical,oof_preds_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.855993352508065\n",
      "25.52635509632409\n",
      "25.750748451581813\n",
      "25.60895946313272\n",
      "25.27113922589319\n",
      "KNN MULTI WEIGHTED LOG LOSS : 25.60271 \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-d05ed96cf16f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'KNN MULTI WEIGHTED LOG LOSS : %.5f '\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mmulti_weighted_logloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_categorical\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moof_preds_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_weighted_logloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_categorical\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moof_preds_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'scores' is not defined"
     ]
    }
   ],
   "source": [
    "#This trains KNN\n",
    "\n",
    "clfs_k = []\n",
    "oof_preds_k = np.zeros((len(full_train_ss), len(classes)))\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y_map, y_map)):\n",
    "    checkPoint = ModelCheckpoint(\"./keras.model\",monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\n",
    "    x_train, y_train = full_train_ss[trn_], y_categorical[trn_]\n",
    "    x_valid, y_valid = full_train_ss[val_], y_categorical[val_]\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=i)\n",
    "\n",
    "    # Train the model using the training sets\n",
    "    model.fit(x_train,y_train)\n",
    "\n",
    "\n",
    "    oof_preds_k[val_, :] = model.predict(x_valid)\n",
    "    print(multi_weighted_logloss(y_valid, model.predict(x_valid)))\n",
    "    clfs_k.append(model)\n",
    "\n",
    "print('KNN MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_categorical,oof_preds_k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This trains NN\n",
    "K.clear_session()\n",
    "def build_model(dropout_rate=0.25,activation='relu'):\n",
    "    start_neurons = 512\n",
    "    # create model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(start_neurons, input_dim=full_train_ss.shape[1], activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//2,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//4,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    \n",
    "    model.add(Dense(start_neurons//8,activation=activation))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_rate/2))\n",
    "    \n",
    "    model.add(Dense(len(classes), activation='softmax'))\n",
    "    return model\n",
    "\n",
    "y_count = Counter(y_map)\n",
    "wtable = np.zeros((len(unique_y),))\n",
    "for i in range(len(unique_y)):\n",
    "    wtable[i] = y_count[i]/y_map.shape[0]\n",
    "\n",
    "def plot_loss_acc(history):\n",
    "    plt.plot(history.history['loss'][1:])\n",
    "    plt.plot(history.history['val_loss'][1:])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('val_loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['acc'][1:])\n",
    "    plt.plot(history.history['val_acc'][1:])\n",
    "    plt.title('model Accuracy')\n",
    "    plt.ylabel('val_acc')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train','Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "clfs_n = []\n",
    "oof_preds_n = np.zeros((len(full_train_ss), len(classes)))\n",
    "epochs = 600\n",
    "batch_size = 100\n",
    "for fold_, (trn_, val_) in enumerate(folds.split(y_map, y_map)):\n",
    "    checkPoint = ModelCheckpoint(\"./keras.model\",monitor='val_loss',mode = 'min', save_best_only=True, verbose=0)\n",
    "    x_train, y_train = full_train_ss[trn_], y_categorical[trn_]\n",
    "    x_valid, y_valid = full_train_ss[val_], y_categorical[val_]\n",
    "    \n",
    "    model = build_model(dropout_rate=0.5,activation='tanh')    \n",
    "    model.compile(loss=mywloss, optimizer='adam', metrics=['accuracy'])\n",
    "    history = model.fit(x_train, y_train,\n",
    "                    validation_data=[x_valid, y_valid], \n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,shuffle=True,verbose=0,callbacks=[checkPoint])       \n",
    "    \n",
    "    plot_loss_acc(history)\n",
    "    \n",
    "    print('Loading Best Model')\n",
    "    model.load_weights('./keras.model')\n",
    "    # # Get predicted probabilities for each class\n",
    "    oof_preds_n[val_, :] = model.predict_proba(x_valid,batch_size=batch_size)\n",
    "    print(multi_weighted_logloss(y_valid, model.predict_proba(x_valid,batch_size=batch_size)))\n",
    "    clfs_n.append(model)\n",
    "    \n",
    "print('MULTI WEIGHTED LOG LOSS : %.5f ' % multi_weighted_logloss(y_categorical,oof_preds_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confidence matrix generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "# Compute confusion matrix for one of the classifiers\n",
    "oof_preds = oof_preds_k\n",
    "cnf_matrix = confusion_matrix(y_map, np.argmax(oof_preds,axis=-1))\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "sample_sub = pd.read_csv('../input/sample_submission.csv')\n",
    "class_names = list(sample_sub.columns[1:-1])\n",
    "del sample_sub;gc.collect()\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure(figsize=(12,12))\n",
    "foo = plot_confusion_matrix(cnf_matrix, classes=class_names,normalize=True,\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtaining test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_test = pd.read_csv('../input/test_set_metadata.csv')\n",
    "\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "chunks = 5000000\n",
    "for i_c, df in enumerate(pd.read_csv('../input/test_set.csv', chunksize=chunks, iterator=True)):\n",
    "    df['flux_ratio_sq'] = np.power(df['flux'] / df['flux_err'], 2.0)\n",
    "    df['flux_by_flux_ratio_sq'] = df['flux'] * df['flux_ratio_sq']\n",
    "    # Group by object id\n",
    "    agg_test = df.groupby('object_id').agg(aggs)\n",
    "    agg_test.columns = new_columns\n",
    "    agg_test['mjd_diff'] = agg_test['mjd_max'] - agg_test['mjd_min']\n",
    "    agg_test['flux_diff'] = agg_test['flux_max'] - agg_test['flux_min']\n",
    "    agg_test['flux_dif2'] = (agg_test['flux_max'] - agg_test['flux_min']) / agg_test['flux_mean']\n",
    "    agg_test['flux_w_mean'] = agg_test['flux_by_flux_ratio_sq_sum'] / agg_test['flux_ratio_sq_sum']\n",
    "    agg_test['flux_dif3'] = (agg_test['flux_max'] - agg_test['flux_min']) / agg_test['flux_w_mean']\n",
    "\n",
    "    del agg_test['mjd_max'], agg_test['mjd_min']\n",
    "#     del df\n",
    "#     gc.collect()\n",
    "    \n",
    "    # Merge with meta data\n",
    "    full_test = agg_test.reset_index().merge(\n",
    "        right=meta_test,\n",
    "        how='left',\n",
    "        on='object_id'\n",
    "    )\n",
    "    full_test[full_train.columns] = full_test[full_train.columns].fillna(train_mean)\n",
    "    full_test_ss = ss.transform(full_test[full_train.columns])\n",
    "    # Make predictions\n",
    "    preds = None\n",
    "    for clf in clfs_n:\n",
    "        if preds is None:\n",
    "            preds = clf.predict_proba(full_test_ss) / folds.n_splits\n",
    "        else:\n",
    "            preds += clf.predict_proba(full_test_ss) / folds.n_splits\n",
    "    \n",
    "   # Compute preds_99 as the proba of class not being any of the others\n",
    "    # preds_99 = 0.1 gives 1.769\n",
    "    preds_99 = np.ones(preds.shape[0])\n",
    "    for i in range(preds.shape[1]):\n",
    "        preds_99 *= (1 - preds[:, i])\n",
    "    \n",
    "    # Store predictions\n",
    "    preds_df = pd.DataFrame(preds, columns=class_names)\n",
    "    preds_df['object_id'] = full_test['object_id']\n",
    "    preds_df['class_99'] = 0.14 * preds_99 / np.mean(preds_99) \n",
    "    \n",
    "    if i_c == 0:\n",
    "        preds_df.to_csv('predictions.csv',  header=True, mode='a', index=False)\n",
    "    else: \n",
    "        preds_df.to_csv('predictions.csv',  header=False, mode='a', index=False)\n",
    "        \n",
    "    del agg_test, full_test, preds_df, preds\n",
    "#     print('done')\n",
    "    if (i_c + 1) % 10 == 0:\n",
    "        print('%15d done in %5.1f' % (chunks * (i_c + 1), (time.time() - start) / 60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = pd.read_csv('predictions.csv')\n",
    "\n",
    "print(z.groupby('object_id').size().max())\n",
    "print((z.groupby('object_id').size() > 1).sum())\n",
    "\n",
    "z = z.groupby('object_id').mean()\n",
    "\n",
    "z.to_csv('single_predictions.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
